<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Untitled.pdf</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 40pt; }
 .s2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 23pt; }
 h1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s3 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 18pt; }
 p { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; margin:0pt; }
</style></head><body><p class="s1" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">ASSIGNMENT 2</p><p class="s2" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Problem 1</p><h1 style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">a- Why is the model invalid?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="411" height="281" alt="image" src="Assignment2_a/Image_001.png"/></span></p><p class="s3" style="text-indent: 0pt;line-height: 20pt;text-align: left;">x</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 94pt;text-indent: 0pt;line-height: 90%;text-align: left;">The layer is invalid due to continuity of the network. We need to add intermediate layers that convert the parameters to the corresponding format.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;line-height: 90%;text-align: left;">b- The classifications you are seeing are almost always wrong. Why is this? What performance should you expect from this particular network, i.e., how often should you expect it to be correct? Is this what you observe?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span><img width="580" height="294" alt="image" src="Assignment2_a/Image_002.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 90%;text-align: left;">It performs badly because of two reasons. The first one -more obvious- is because of the random weights applied to the operation. The second is due to the small amount of units in the FC layer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 10pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Problem 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="503" height="256" alt="image" src="Assignment2_a/Image_003.jpg"/></span></p><h1 style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;line-height: 90%;text-align: justify;">a- What accuracy do you observe in training MNIST? How many inferences per second does the demo perform? How many examples per second does it train? Then try the same thing with Fashion MNIST and document your findings.</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 407pt;text-indent: 0pt;line-height: 90%;text-align: left;">On MINST, It performs better since the network has been trained and the inferences raised from 1050 / sec to approximately 1600/sec with 1460 examples/ second/.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 17pt;text-indent: 0pt;line-height: 90%;text-align: left;">Start training and you should see the accuracy plummet to zero, with terrible results. Whatâ€™s going on? Hint: Notice that many of the probabilities will print as Nan%. Document these results and write up your ideas for why this happens.</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 18pt;text-indent: 0pt;line-height: 90%;text-align: left;">By applying multiple linearly activated layers to our NN makes it worst because no matter how many layers or neurons to our network the result will be always a linear function. Each layer would get the weighted values from the previous linear function and will calculate a weighted sum on that input - it will fire every time on another linear activation function.</p><h1 style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 90%;text-align: left;">Train the new model. How well does it perform? Then make the first FC model wider by increasing the number of units to 100. Does this make a difference? Document the results for these questions on your webpage.</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 71pt;text-indent: 0pt;text-align: left;"><span><img width="828" height="419" alt="image" src="Assignment2_a/Image_004.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 6pt;text-indent: 0pt;line-height: 90%;text-align: left;">The model starts really bad at the beginning, slowly increasing the accuracy over time. The results are as low as 37% avg. I wonder if this is produced because of the RELU activating only too little amount of neurons.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: left;"><span><img width="790" height="414" alt="image" src="Assignment2_a/Image_005.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;line-height: 90%;text-align: justify;">The model performs better with accuracies over 98 % (2 minutes training) Introducing RELU and adding more units to the first FC layer makes a difference in terms of speed and &#39;&#39;lightness&#39; of the network because of its outputs producing -I guess- a sparse activation of the neurons (if some are 0 then they don&#39;t activate). Therefore is less computationally expensive due to the use of a simpler mathematical operations.</p></body></html>
