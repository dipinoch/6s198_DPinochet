<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Untitled.pdf</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .s1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 40pt; }
 .s2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 23pt; }
 h1 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s3 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 18pt; }
 p { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; margin:0pt; }
 .s4 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 h2 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .s5 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li:before {counter-increment: c1; content: counter(c1, lower-latin)"- "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l1> li:first-child:before {counter-increment: c1 0;  }
 #l2 {padding-left: 0pt;counter-reset: c2 1; }
 #l2> li:before {counter-increment: c2; content: counter(c2, decimal)". "; color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 #l2> li:first-child:before {counter-increment: c2 0;  }
</style></head><body><p class="s1" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">ASSIGNMENT 2</p><p class="s2" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Problem 1</p><h1 style="padding-top: 3pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">a- Why is the model invalid?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="411" height="281" alt="image" src="assignment2_a/Image_001.png"/></span></p><p class="s3" style="text-indent: 0pt;line-height: 20pt;text-align: left;">x</p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-left: 94pt;text-indent: 0pt;line-height: 90%;text-align: left;">The layer is invalid due to continuity of the network. We need to add intermediate layers that convert the parameters to the corresponding format.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 5pt;padding-left: 19pt;text-indent: 0pt;line-height: 90%;text-align: left;">b- The classifications you are seeing are almost always wrong. Why is this? What performance should you expect from this particular network, i.e., how often should you expect it to be correct? Is this what you observe?</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span><img width="580" height="294" alt="image" src="assignment2_a/Image_002.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 90%;text-align: left;">It performs badly because of two reasons. The first one -more obvious- is because of the random weights applied to the operation. The second is due to the small amount of units in the FC layer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 10pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">Problem 2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="503" height="256" alt="image" src="assignment2_a/Image_003.jpg"/></span></p><ol id="l1"><li style="padding-top: 5pt;padding-left: 13pt;text-indent: 0pt;line-height: 90%;text-align: justify;"><h1 style="display: inline;">What accuracy do you observe in training MNIST? How many inferences per second does the demo perform? How many examples per second does it train? Then try the same thing with Fashion MNIST and document your findings.</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 407pt;text-indent: 0pt;line-height: 90%;text-align: left;">On MINST, It performs better since the network has been trained and the inferences raised from 1050 / sec to approximately 1600/sec with 1460 examples/ second/.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 17pt;text-indent: 0pt;line-height: 90%;text-align: left;">Start training and you should see the accuracy plummet to zero, with terrible results. Whatâ€™s going on? Hint: Notice that many of the probabilities will print as Nan%. Document these results and write up your ideas for why this happens.</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 5pt;padding-left: 18pt;text-indent: 0pt;line-height: 90%;text-align: left;">By applying multiple linearly activated layers to our NN makes it worst because no matter how many layers or neurons to our network the result will be always a linear function. Each layer would get the weighted values from the previous linear function and will calculate a weighted sum on that input - it will fire every time on another linear activation function.</p><h1 style="padding-top: 4pt;padding-left: 11pt;text-indent: 0pt;line-height: 90%;text-align: left;">Train the new model. How well does it perform? Then make the first FC model wider by increasing the number of units to 100. Does this make a difference? Document the results for these questions on your webpage.</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 77pt;text-indent: 0pt;text-align: left;"><span><img width="828" height="419" alt="image" src="assignment2_a/Image_004.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 12pt;text-indent: 0pt;line-height: 90%;text-align: left;">The model starts really bad at the beginning, slowly increasing the accuracy over time. The results are as low as 37% avg. I wonder if this is produced because of the RELU activating only too little amount of neurons.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span><img width="790" height="414" alt="image" src="assignment2_a/Image_005.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 90%;text-align: justify;">The model performs better with accuracies over 98 % (2 minutes training) Introducing RELU and adding more units to the first FC layer makes a difference in terms of speed and &#39;&#39;lightness&#39; of the network because of its outputs producing -I guess- a sparse activation of the neurons (if some are 0 then they don&#39;t activate). Therefore is less computationally expensive due to the use of a simpler mathematical operations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li style="padding-top: 5pt;padding-left: 18pt;text-indent: 0pt;line-height: 90%;text-align: left;"><h1 style="display: inline;">Train your MNIST model with 1,2,3,4, and 5 FC layers, with ReLU between them. For each, use the same hyperparameters, and the same number of hidden units (except for the last layer). What were the training times and accuracy? Do you see any overfitting? What can you conclude about how many layers to use? Include screenshots of the Training Stats for each of your examples.</h1></li></ol></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="703" height="363" alt="image" src="assignment2_a/Image_006.jpg"/></span></p><p style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">CASE 1: INPUT &gt; FLATTEN &gt; FC(100) &gt; RELU &gt; SoftMax &gt; Label - Training time 120 sec.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 90%;text-align: left;">The model has an accuracy close to 91%, with an average of 1450 inferences per second and approximately 1400 Examples per second.</p><p class="s4" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">CASE 1: INPUT &gt; FLATTEN &gt; FC(100) &gt; RELU &gt; SoftMax &gt; Label - Training time 120 sec. approx</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span><img width="597" height="308" alt="image" src="assignment2_a/Image_007.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-top: 6pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">CASE 2: INPUT &gt; FLATTEN &gt; FC(100) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; SoftMax &gt; Label - Training time 120 sec. approx</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 12pt;text-indent: 0pt;line-height: 90%;text-align: left;">The model has an accuracy close to 91%, with an average of 1450 inferences per second and approximately 1400 Examples per second.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 4pt;text-indent: 0pt;line-height: 90%;text-align: left;">The model performs slower and compared to the previous one it takes longer in getting good results. after 90 seconds it reaches good results. with less inferences and less examples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="591" height="284" alt="image" src="assignment2_a/Image_008.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><span><img width="597" height="307" alt="image" src="assignment2_a/Image_009.jpg"/></span></p><p class="s4" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">CASE 3: INPUT &gt; FLATTEN &gt; FC(100) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; SoftMax &gt; Label - Training time 120 sec. approx</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 540pt;text-indent: 0pt;line-height: 90%;text-align: left;">by adding one more FC layer and another RELU in between, it performs slightly better than the previous one.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">CASE 4: INPUT &gt; FLATTEN &gt; FC(100) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; SoftMax &gt; Label - Training time 120 sec. approx</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span><img width="587" height="307" alt="image" src="assignment2_a/Image_010.jpg"/></span></p><p style="padding-left: 540pt;text-indent: 0pt;line-height: 90%;text-align: justify;">Adding another FC and RELU layers makes it worst and slower in terms of both accuracy and speed.</p><p style="padding-top: 10pt;text-indent: 0pt;text-align: right;">Probably it shows signs of overfitting.</p><p style="text-indent: 0pt;text-align: left;"><span><img width="682" height="356" alt="image" src="assignment2_a/Image_011.jpg"/></span></p><p class="s4" style="padding-top: 4pt;padding-left: 12pt;text-indent: 0pt;text-align: left;">CASE 4: INPUT &gt; FLATTEN &gt; FC(100) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; FC(10) &gt; RELU &gt; SoftMax &gt; Label - Training time 120 sec. approx</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 544pt;text-indent: 0pt;line-height: 90%;text-align: left;">Finally , adding a 5th FC layer, shows that the model performs worst because is overfitting.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 544pt;text-indent: 0pt;line-height: 90%;text-align: justify;">In my opinion and based on results, using 3 FC layers with RELU in between performed better than other configurations.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 8pt;padding-left: 16pt;text-indent: 0pt;line-height: 90%;text-align: left;">Build a model with 3 FC layers, with ReLU between them. Try making the first layer wide and the second narrow, and vice versa, using the same hyperparameters as before. Which performs better? Why do you think this is?</h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;line-height: 90%;text-align: left;">Making the first layer wider (FC(300)) , the NN performs well and fast enough</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="459" height="242" alt="image" src="assignment2_a/Image_012.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 17pt;text-indent: 0pt;line-height: 90%;text-align: left;">Results are not very good when using the same configuration in Fashion MNIST.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="470" height="244" alt="image" src="assignment2_a/Image_013.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 29pt;text-indent: 0pt;line-height: 90%;text-align: left;">Repeating the same configuration on CIFAR-10, it performs really bad.</p><p style="padding-top: 7pt;padding-left: 61pt;text-indent: 0pt;line-height: 90%;text-align: left;">Inverting the number of units , makes que second NN perform really bad. My guess is that this is due to the small number of neurons in the first layer, the NN doesn&#39;t &#39;learn enough&#39; to pass the values to the next layer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span><img width="449" height="232" alt="image" src="assignment2_a/Image_014.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 28pt;text-indent: 0pt;line-height: 90%;text-align: left;">Changing the hidden units number with the same configuration doesn&#39;t make a significant difference in terms of performance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span><img width="512" height="267" alt="image" src="assignment2_a/Image_015.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 40pt;text-indent: 0pt;line-height: 90%;text-align: left;">Inverting the number of hidden units doesn&#39;t make a difference. Maybe because tensors are more complex than the other 2 datasets</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s5" style="padding-left: 25pt;text-indent: 0pt;text-align: left;"><span><img width="471" height="243" alt="image" src="assignment2_a/Image_016.jpg"/></span>	<span><img width="455" height="234" alt="image" src="assignment2_a/Image_017.jpg"/></span></p></body></html>
