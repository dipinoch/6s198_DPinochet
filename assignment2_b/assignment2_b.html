<!doctype html>
<html>
<head>
<meta charset="UTF-8">
<title>Assignment_2b</title>
<style type="text/css">
.Paragraph {
	font-family: Consolas, Andale Mono, Lucida Console, Lucida Sans Typewriter, Monaco, Courier New, monospace;
}
.Paragraph p {
	font-family: Consolas, Andale Mono, Lucida Console, Lucida Sans Typewriter, Monaco, Courier New, monospace;
}
</style>
</head>

<body class="Paragraph">
<p><strong>PROBLEM 6	</strong></p>
<p><strong>1- Look at some of the testing results and try to find examples of classifications where the system does poorly and is even wrong. When you see interesting results, document them on your webpage. </strong></p>
<p>Answer- training with default values gives a really bad performance with a very poor accuracy. In the case of more complex digits such as 8s and 3s and in general curved digits, it has real problems performing in a good way with the default parameters. </p>
<p><img src="..IMG/Screen Shot 2018-09-16 at 23.11.05.png" width="937" height="543" alt=""/></p>
<p><a href="code/index_Default_settings.js" title="Code Default Settings" target="new">Code</a></p>
<p dir="ltr"><strong id="docs-internal-guid-f56ffb25-7fff-e8b7-9df1-66b8f2f8e618">2. Experiment with changing the batch size and the number of batches to try to improve the testing results. Give a brief description of what you tried, and the results.</strong></p>
<p>Answer- By changing the default settings increasing the number of batch size to 50 and the number of batches to 200, we see a good improvement where the accuracy rises up to 90% and the erroneal predictions are way less than before. By giving more images for the model to see before updating values and adding more batches to train the model (50 and 200 respectively) shows that tunning those values could improve even more the performance. </p>
<p><img src="..IMG/Screen Shot 2018-09-16 at 23.16.26.png" width="905" height="588" alt=""/></p>
<p>By changing other parameters and trying configurations always return similar results. </p>
<div>
  <div>const BATCH_SIZE =64;</div>
  <br>
  <div>
    <p>const NUM_BATCHES =200;
      </p><div>
      <div>const TEST_BATCH_SIZE = 60;</div>
      <div></div>
      <div>const TEST_ITERATION_FREQUENCY = 5;</div>
    </div>
  </div>
</div>
<p>&nbsp;</p>
<p><a href="code/index_hyperparameers_mod.js" title="Code Hyperparameters modified" target="new">Code</a></p>
<p>&nbsp;</p>
<p><strong>3 - Constructing a model. </strong></p>
<p><strong>3a- trying with a simple model just like assignment 2a</strong></p>
<p>the layer configuration of the models is: </p>
<div>
  <div>const model = tf.sequential();</div>
  <br>
  <div>// First flatten the image</div>
  <div>// First layer must have an input shape defined.</div>
  <div>model.add(tf.layers.flatten({</div>
  <div> inputShape: image_shape</div>
  <div>}))</div>
  <br>
  <div>// Add a fully conected (dense) layer </div>
  <div>model.add(tf.layers.dense({</div>
  <div> units: 100, </div>
  <div> kernelInitializer: 'varianceScaling'</div>
  <div>}));</div>
  <br>
  <div>//Add a Relu </div>
  <div>model.add(tf.layers.dense({</div>
  <div> units:10,</div>
  <div> activation:'relu',</div>
  <div> kernelInitializer: 'varianceScaling'</div>
  <div> }));</div>
  <br>
  <div>//Add a fully conected (dense) layer </div>
  <div>model.add(tf.layers.dense({</div>
  <div> units: 10, </div>
  <div> kernelInitializer: 'varianceScaling'</div>
  <div> }));</div>
  <div> </div>
  <div>//Add a Relu </div>
  <div>model.add(tf.layers.dense({</div>
  <div> units:10,</div>
  <div> activation:'relu',</div>
  <div> kernelInitializer: 'varianceScaling'</div>
  <div> }));</div>
  <br>
  <br>
  <br>
  <div>//Add softmax</div>
  <div>model.add(tf.layers.softmax());</div>
  <br>
  <div>model.compile({</div>
  <div> optimizer: 'sgd', </div>
  <div> loss: 'categoricalCrossentropy',</div>
  <div> metrics: ['accuracy']</div>
  <div>});</div>
</div>
<p>The results show an increment in the accuracy and a decrease in the loss values with the same hyperparameters than before. </p>
<div>const BATCH_SIZE =64;</div>
<br>
<div>
  <p>const NUM_BATCHES =200; </p>
  <div>
    <div>const TEST_BATCH_SIZE = 60;</div>
    <div></div>
    <div>const TEST_ITERATION_FREQUENCY = 5;</div>
  </div>
</div>
<p><img src="..IMG/Screen Shot 2018-09-16 at 23.39.59.png" width="907" height="413" alt=""/></p>
<p>&nbsp;</p>
<p><a href="code/index_layersModA.js" title="Code Layers MOD a" target="new">Code</a><br>
</p>
<p>&nbsp;</p>
<p><strong>3b- Playing with more sofisticated configurations using conv2D layers. </strong></p>
<p><strong>Can the model perform even better than this? Right now adding more fully connected layers and RELU indeed make the performance better than the default model but if we want to obtain really good results, we need to try more complex configurations such as a convolutional image classifier model</strong></p>
<p>the layer configuration of the models is: </p>
<div>
  <div>const model = tf.sequential();</div>
  <br>
  <div>//add the first convolutional layer </div>
  <div>model.add(tf.layers.conv2d({</div>
  <div> inputShape: [28, 28, 1],</div>
  <div> kernelSize: 5,</div>
  <div> filters: 8,</div>
  <div> strides: 1,</div>
  <div> activation: 'relu',</div>
  <div> kernelInitializer: 'VarianceScaling'</div>
  <div>}));</div>
  <br>
  <div>//add maxpool</div>
  <div>model.add(tf.layers.maxPooling2d({</div>
  <div> poolSize: [2, 2],</div>
  <div> strides: [2, 2]</div>
  <div>}));</div>
  <br>
  <div>//add second conv2D</div>
  <div>model.add(tf.layers.conv2d({</div>
  <div> kernelSize: 5,</div>
  <div> filters: 16,</div>
  <div> strides: 1,</div>
  <div> activation: 'relu',</div>
  <div> kernelInitializer: 'VarianceScaling'</div>
  <div>}));</div>
  <br>
  <div>//add maxpool</div>
  <div>model.add(tf.layers.maxPooling2d({</div>
  <div> poolSize: [2, 2],</div>
  <div> strides: [2, 2]</div>
  <div>}));</div>
  <br>
  <div>//add flatten</div>
  <div>model.add(tf.layers.flatten());</div>
  <br>
  <div>//finally add a dense layer</div>
  <div>model.add(tf.layers.dense({</div>
  <div> units: 10,</div>
  <div> kernelInitializer: 'VarianceScaling',</div>
  <div> activation: 'softmax'</div>
  <div>}));</div>
  <br>
  <div>model.compile({</div>
  <div> optimizer: optimizer,</div>
  <div> loss: 'categoricalCrossentropy',</div>
  <div> metrics: ['accuracy'],</div>
  <div>});</div>
</div>
<p>the hyperparametrs were the following</p>
<div>
  <div>const BATCH_SIZE =128;</div>
  <br>
  <div>const NUM_BATCHES =500;</div>
  <br>
  <div></div>
  <div>const DO_TESTING = true;</div>
  <div></div>
  <div></div>
  <div>const TEST_BATCH_SIZE = 1000;</div>
  <div></div>
  <div>const TEST_ITERATION_FREQUENCY = 5;</div>
</div>
<p>Using these model and this set of Hyperparameters, the results are really good! (I followed the configuration that tensorflow.js page do for MNIST). </p>
<p><img src="..IMG/Screen Shot 2018-09-17 at 00.04.58.png" width="855" height="452" alt=""/></p>
<p><a href="code/index_layersModB.js" title="CODE LAYER MOD B" target="new">CODE</a></p>
<p>&nbsp;</p>
<p>SAme code and configuration applied to the Fashion -Mnist mdoel shows worst results. </p>
<p><img src="..IMG/Screen Shot 2018-09-17 at 00.23.34.png" width="884" height="428" alt=""/></p>
<p>&nbsp;</p>
<p>By using the same model with the CIFAR-10 Dataset, the results are really bad. the model overfits and can perform in a decent way. </p>
<p><img src="..IMG/Screen Shot 2018-09-17 at 00.14.13.png" width="901" height="429" alt=""/></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</body>
</html>
